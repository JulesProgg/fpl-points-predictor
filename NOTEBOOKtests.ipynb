{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82156bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ajoute la racine du projet (/files/fpl-points-predictor) au PYTHONPATH\n",
    "PROJECT_ROOT = Path(\"/files/fpl-points-predictor\")\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "import src.data_pipeline as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_select_useful_drops_unwanted_columns():\n",
    "    \"\"\"select_useful must drop unuseful columns and keep the ordrer of USEFUL_COLS.\"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2],\n",
    "            \"name\": [\"Player A\", \"Player B\"],\n",
    "            \"team\": [\"Team X\", \"Team Y\"],\n",
    "            \"position\": [\"FWD\", \"MID\"],\n",
    "            \"minutes\": [1000, 2000],\n",
    "            \"junk\": [\"foo\", \"bar\"],  # unuseful column\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = dp.select_useful(df)\n",
    "\n",
    "    # 1) the columns\"junk\" must disappear \n",
    "    assert \"junk\" not in result.columns\n",
    "\n",
    "    # 2) the existing columns of df should be in the order defined in USEFUL_COLS,\n",
    "    expected_cols = [c for c in dp.USEFUL_COLS if c in df.columns]\n",
    "    assert list(result.columns) == expected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da42606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_select_useful_handles_missing_useful_columns():\n",
    "    \"\"\"select_useful should work even if some columns don't exist\"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"name\": [\"Player A\"],\n",
    "            \"team\": [\"Team X\"],\n",
    "            # only this two columns are fulfilled\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = dp.select_useful(df)\n",
    "\n",
    "    # it must keep ONLY the defined columns in USEFUL_COLS\n",
    "    assert list(result.columns) == [\"name\", \"team\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40eff642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_pipeline_reads_three_csvs_and_writes_output(tmp_path, monkeypatch):\n",
    "    \"\"\"\n",
    "    run_pipeline must :\n",
    "    - read 3 CSV (one season each),\n",
    "    - normalise the dataset 24-25 (first/second → name, etc.),\n",
    "    - concatenate 3 of them,\n",
    "    - write a CSSV in DATA_PROCESSED_DIR,\n",
    "    - return the path of the file.\n",
    "    We mock pd.read_csv and redirect DATA_PROCESSED_DIR to a temporary directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Prepare fake DataFrames for each season\n",
    "    df22 = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1],\n",
    "            \"name\": [\"Player 22\"],\n",
    "            \"team\": [\"Team 22\"],\n",
    "            \"position\": [\"FWD\"],\n",
    "            \"minutes\": [1000],\n",
    "            \"total_points\": [150],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df23 = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [2],\n",
    "            \"name\": [\"Player 23\"],\n",
    "            \"team\": [\"Team 23\"],\n",
    "            \"position\": [\"MID\"],\n",
    "            \"minutes\": [2000],\n",
    "            \"total_points\": [160],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Dataset 24-25 with the special structure which we normalise with run_pipeline\n",
    "    df24 = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [3],\n",
    "            \"first_name\": [\"Erling\"],\n",
    "            \"second_name\": [\"Haaland\"],\n",
    "            \"player_position\": [\"FWD\"],\n",
    "            \"team_name\": [\"Team 24\"],\n",
    "            \"minutes\": [2500],\n",
    "            \"total_points\": [200],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b87db1af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monkeypatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df24.copy()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected path in fake_read_csv: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mmonkeypatch\u001b[49m.setattr(dp.pd, \u001b[33m\"\u001b[39m\u001b[33mread_csv\u001b[39m\u001b[33m\"\u001b[39m, fake_read_csv)\n",
      "\u001b[31mNameError\u001b[39m: name 'monkeypatch' is not defined"
     ]
    }
   ],
   "source": [
    "    # 2) Mock of pd.read_csv to return the above dataFrames\n",
    "    def fake_read_csv(path, *args, **kwargs):\n",
    "        path_str = str(path)\n",
    "        if \"season22-23.csv\" in path_str:\n",
    "            return df22.copy()\n",
    "        if \"season23-24.csv\" in path_str:\n",
    "            return df23.copy()\n",
    "        if \"season24-25.csv\" in path_str:\n",
    "            return df24.copy()\n",
    "        raise ValueError(f\"Unexpected path in fake_read_csv: {path_str}\")\n",
    "\n",
    "    monkeypatch.setattr(dp.pd, \"read_csv\", fake_read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c2083",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monkeypatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3) Rediriger DATA_PROCESSED_DIR vers un dossier temporaire de pytest\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmonkeypatch\u001b[49m.setattr(dp, \u001b[33m\"\u001b[39m\u001b[33mDATA_PROCESSED_DIR\u001b[39m\u001b[33m\"\u001b[39m, tmp_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'monkeypatch' is not defined"
     ]
    }
   ],
   "source": [
    "# 3) Redirect DATA_PROCESSED_DIR to a temporary pytest folder\n",
    "monkeypatch.setattr(dp, \"DATA_PROCESSED_DIR\", tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afa4a5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m out_path = dp.run_pipeline()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# The return path must match the file in tmp_path\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m expected_path = \u001b[43mtmp_path\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mplayers_all_seasons.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m out_path == expected_path\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m out_path.exists()\n",
      "\u001b[31mNameError\u001b[39m: name 'tmp_path' is not defined"
     ]
    }
   ],
   "source": [
    "# 4) Run the pipeline\n",
    "out_path = dp.run_pipeline()\n",
    "\n",
    "# The return path must match the file in tmp_path\n",
    "expected_path = tmp_path / \"players_all_seasons.csv\"\n",
    "assert out_path == expected_path\n",
    "assert out_path.exists()\n",
    "\n",
    "# 5) Check the contents of the written CSV file\n",
    "full = pd.read_csv(out_path)\n",
    "\n",
    "# There should be 3 lines (one per season)\n",
    "assert len(full) == 3\n",
    "\n",
    "# All columns in the CSV must be a subset of USEFUL_COLS\n",
    "assert all(col in dp.USEFUL_COLS for col in full.columns)\n",
    "\n",
    "# Player 24-25 must have a constructed name ‘Erling Haaland’ for example\n",
    "assert \"name\" in full.columns\n",
    "assert \"Erling Haaland\" in full[\"name\"].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
